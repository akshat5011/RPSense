{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5104,
     "status": "ok",
     "timestamp": 1754538086261,
     "user": {
      "displayName": "Akshat",
      "userId": "02656238651709843440"
     },
     "user_tz": -330
    },
    "id": "4KZs-sX1dAW8",
    "outputId": "4ce6933d-9d39-4460-faa6-a3ff1c6ef761"
   },
   "outputs": [],
   "source": [
    "!pip install -q flask pyngrok\n",
    "from pyngrok import ngrok\n",
    "\n",
    "# Replace this with your actual token from the ngrok dashboard\n",
    "ngrok.set_auth_token(\"your-ngrok-auth-token\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 24601,
     "status": "ok",
     "timestamp": 1754538219395,
     "user": {
      "displayName": "Akshat",
      "userId": "02656238651709843440"
     },
     "user_tz": -330
    },
    "id": "QZ7aj4hqfBF7",
    "outputId": "52d7827a-f113-486d-ac16-b5d2389a2aed"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 4141,
     "status": "ok",
     "timestamp": 1754538141358,
     "user": {
      "displayName": "Akshat",
      "userId": "02656238651709843440"
     },
     "user_tz": -330
    },
    "id": "y9ICGoTqiFGS",
    "outputId": "86f2f050-f639-43a5-e2c6-92bc7908eae5"
   },
   "outputs": [],
   "source": [
    "# prompt: unzip rar file /content/RPSENSE BACKEND.rar\n",
    "\n",
    "!sudo apt-get install unrar\n",
    "# !unrar x \"/content/RPSENSE BACKEND.rar\" \"/content/\"\n",
    "!unrar x \"/content/bacnew.rar\" \"/content/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 21466,
     "status": "ok",
     "timestamp": 1754538180672,
     "user": {
      "displayName": "Akshat",
      "userId": "02656238651709843440"
     },
     "user_tz": -330
    },
    "id": "dxlQd864go9b",
    "outputId": "703768cb-516d-420f-e9ec-3a3d23be1431"
   },
   "outputs": [],
   "source": [
    "# ! pip install flask_cors python-dotenv mediapipe flask_socketio\n",
    "! pip install flask_cors python-dotenv mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 5973,
     "status": "ok",
     "timestamp": 1754538225372,
     "user": {
      "displayName": "Akshat",
      "userId": "02656238651709843440"
     },
     "user_tz": -330
    },
    "id": "RU33lhv_dkm3",
    "outputId": "616eae5a-77a6-4121-8d77-4a909381aacf"
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade flask-socketio python-socketio python-engineio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3229,
     "status": "ok",
     "timestamp": 1754418281172,
     "user": {
      "displayName": "Akshat",
      "userId": "02656238651709843440"
     },
     "user_tz": -330
    },
    "id": "7LDZVtbpP9Em",
    "outputId": "89986cbf-70eb-4291-a1ba-73c8b8620bf3"
   },
   "outputs": [],
   "source": [
    "!pip list | grep socketio\n",
    "!pip list | grep engineio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "id": "ZbROG7S3gutY",
    "outputId": "cfe713e8-5a34-4562-9a2f-799f747472ba"
   },
   "outputs": [],
   "source": [
    "# !python \"/content/RPSENSE BACKEND/app.py\"\n",
    "!python \"/content/bacnew/app.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pZOJdlpYhCpM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XX_L3uwpf9hk"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "class Config:\n",
    "    # Base paths\n",
    "    MODEL_PATH = '/content/drive/MyDrive/RPSense_Dataset/finetuned_after100layers_mobilenetv2_rpsense.h5'\n",
    "\n",
    "    # Model configuration\n",
    "    MODEL_INPUT_SIZE = (224, 224)\n",
    "    CLASSES = ['invalid', 'paper', 'rock', 'scissors']\n",
    "    CONFIDENCE_THRESHOLD = 0.75\n",
    "\n",
    "    # Frame processing configuration\n",
    "    INFERENCE_WINDOW_DURATION = 2.0  # seconds\n",
    "    FRAMES_PER_SECOND = 10  # Expected frames per second from frontend\n",
    "    MAX_FRAMES_IN_WINDOW = int(INFERENCE_WINDOW_DURATION * FRAMES_PER_SECOND)\n",
    "\n",
    "    # MediaPipe configuration\n",
    "    HAND_DETECTION_CONFIDENCE = 0.5\n",
    "    HAND_TRACKING_CONFIDENCE = 0.5\n",
    "    MAX_HANDS = 2  # We'll check if more than 1 hand is detected\n",
    "\n",
    "    # Server configuration\n",
    "    DEBUG = True\n",
    "    HOST = '0.0.0.0'\n",
    "    PORT = 5000\n",
    "\n",
    "    # Image processing\n",
    "    HAND_BBOX_PADDING = 30  # Pixels to add around detected hand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "loL7MpWEzpyY"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "def decode_frame_from_base64(base64_string):\n",
    "    \"\"\"Decode base64 string to OpenCV image\"\"\"\n",
    "    try:\n",
    "        # Remove data URL prefix if present\n",
    "        if \"data:image\" in base64_string:\n",
    "            base64_string = base64_string.split(\",\")[1]\n",
    "\n",
    "        # Decode base64 to bytes\n",
    "        img_bytes = base64.b64decode(base64_string)\n",
    "\n",
    "        # Convert to numpy array\n",
    "        nparr = np.frombuffer(img_bytes, np.uint8)\n",
    "\n",
    "        # Decode to OpenCV image\n",
    "        img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)\n",
    "\n",
    "        return img # BGR color image\n",
    "    except Exception as e:\n",
    "        print(f\"Error decoding frame: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def encode_frame_to_base64(img):\n",
    "    \"\"\"Encode OpenCV image to base64 string\"\"\"\n",
    "    try:\n",
    "        # Encode image to JPEG\n",
    "        _, buffer = cv2.imencode(\".jpg\", img, [cv2.IMWRITE_JPEG_QUALITY, 90])\n",
    "\n",
    "        # Convert to base64\n",
    "        img_base64 = base64.b64encode(buffer).decode(\"utf-8\")\n",
    "\n",
    "        return f\"data:image/jpeg;base64,{img_base64}\"\n",
    "    except Exception as e:\n",
    "        print(f\"Error encoding frame: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def extract_hand_roi(image, hand_landmarks, padding=20):\n",
    "    \"\"\"Extract hand region of interest from image using MediaPipe landmarks\"\"\"\n",
    "    if hand_landmarks is None or len(hand_landmarks.landmark) == 0:\n",
    "        return None, None\n",
    "\n",
    "    h, w, _ = image.shape\n",
    "\n",
    "    x_coords = [landmark.x * w for landmark in hand_landmarks.landmark]\n",
    "    y_coords = [landmark.y * h for landmark in hand_landmarks.landmark]\n",
    "\n",
    "    x_min = max(0, int(min(x_coords)) - padding)\n",
    "    x_max = min(w, int(max(x_coords)) + padding)\n",
    "    y_min = max(0, int(min(y_coords)) - padding)\n",
    "    y_max = min(h, int(max(y_coords)) + padding)\n",
    "\n",
    "    if x_max - x_min <= 0 or y_max - y_min <= 0:\n",
    "        return None, None\n",
    "\n",
    "    roi = image[y_min:y_max, x_min:x_max]\n",
    "    return roi, (x_min, y_min, x_max, y_max)\n",
    "\n",
    "\n",
    "\n",
    "def draw_prediction_overlay(image, bbox, prediction, confidence):\n",
    "    \"\"\"Draw prediction overlay on image\"\"\"\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "\n",
    "    # Define colors for each class\n",
    "    colors = {\n",
    "        \"rock\": (0, 255, 0),  # Green\n",
    "        \"paper\": (255, 0, 0),  # Blue\n",
    "        \"scissors\": (0, 0, 255),  # Red\n",
    "        \"invalid\": (128, 128, 128),  # Gray\n",
    "    }\n",
    "\n",
    "    color = colors.get(prediction, (255, 255, 255))\n",
    "\n",
    "    # Draw bounding box\n",
    "    cv2.rectangle(image, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "\n",
    "    # Draw label background\n",
    "    label = f\"{prediction.upper()}: {confidence:.2f}\"\n",
    "    (label_width, label_height), _ = cv2.getTextSize(\n",
    "        label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 2\n",
    "    )\n",
    "\n",
    "    cv2.rectangle(\n",
    "        image,\n",
    "        (x_min, y_min - label_height - 10),\n",
    "        (x_min + label_width, y_min),\n",
    "        color,\n",
    "        -1,\n",
    "    )\n",
    "\n",
    "    # Draw label text\n",
    "    cv2.putText(\n",
    "        image,\n",
    "        label,\n",
    "        (x_min, y_min - 5),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX,\n",
    "        0.6,\n",
    "        (255, 255, 255),\n",
    "        2,\n",
    "    )\n",
    "\n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zrjryvoPfZSL"
   },
   "outputs": [],
   "source": [
    "import mediapipe as mp\n",
    "import cv2\n",
    "\n",
    "class HandDetector:\n",
    "    def __init__(self):\n",
    "        self.mp_hands = mp.solutions.hands\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "        self.hands = self.mp_hands.Hands(\n",
    "            static_image_mode=False,\n",
    "            max_num_hands=Config.MAX_HANDS,\n",
    "            min_detection_confidence=Config.HAND_DETECTION_CONFIDENCE,\n",
    "            min_tracking_confidence=Config.HAND_TRACKING_CONFIDENCE\n",
    "        )\n",
    "\n",
    "    def detect_hands(self, image):\n",
    "        \"\"\"\n",
    "        Detect hands in image\n",
    "        Returns: (status, message, results)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Convert BGR to RGB\n",
    "            rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Process the image\n",
    "            results = self.hands.process(rgb_image)\n",
    "\n",
    "            if not results.multi_hand_landmarks:\n",
    "                return \"no_hands\", \"No hands detected\", None\n",
    "\n",
    "            # Check for multiple hands\n",
    "            if len(results.multi_hand_landmarks) > 1:\n",
    "                return \"invalid\", \"Multiple hands detected\", None\n",
    "\n",
    "            # Single hand detected\n",
    "            return \"success\", \"Single hand detected\", results.multi_hand_landmarks[0]\n",
    "\n",
    "        except Exception as e:\n",
    "            return \"error\", f\"Hand detection error: {str(e)}\", None\n",
    "\n",
    "    def __del__(self):\n",
    "        if hasattr(self, 'hands'):\n",
    "            self.hands.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 9102,
     "status": "ok",
     "timestamp": 1752343040862,
     "user": {
      "displayName": "Akshat",
      "userId": "02656238651709843440"
     },
     "user_tz": -330
    },
    "id": "5mDHGVPAgn11",
    "outputId": "1914aa6d-f7b4-481c-b0a0-4cee719c3646"
   },
   "outputs": [],
   "source": [
    "# Load the image\n",
    "image_path = \"/content/RPSENSE BACKEND/WIN_20250712_22_09_29_Pro.jpg\"\n",
    "image = cv2.imread(image_path)\n",
    "detector = HandDetector()\n",
    "\n",
    "if image is None:\n",
    "    print(f\"Error: Could not load image from {image_path}\")\n",
    "else:\n",
    "    # Detect hands in the loaded image\n",
    "    status, message, hand_landmarks = detector.detect_hands(image.copy()) # Use a copy to avoid modifying original\n",
    "\n",
    "    print(f\"Detection Status: {status}\")\n",
    "    print(f\"Detection Message: {message}\")\n",
    "    print(f\"Hand Landmarks: {hand_landmarks}\")\n",
    "\n",
    "    # Optionally draw landmarks on the image and display it\n",
    "    if status == \"success\":\n",
    "        # Draw landmarks\n",
    "        detector.mp_drawing.draw_landmarks(\n",
    "            image, hand_landmarks, detector.mp_hands.HAND_CONNECTIONS)\n",
    "\n",
    "        # Display the image (only works in environments like local Jupyter with imshow)\n",
    "        # In Colab, you'd typically save the image or display it differently\n",
    "        from google.colab.patches import cv2_imshow\n",
    "        cv2_imshow(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39LCSCjWidw2"
   },
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "joMXhoIzifLH"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "\n",
    "\n",
    "class ImagePreprocessor:\n",
    "    def __init__(self):\n",
    "        self.input_size = Config.MODEL_INPUT_SIZE\n",
    "\n",
    "    def preprocess_for_model(self, roi_image):\n",
    "        \"\"\"\n",
    "        Preprocess hand ROI for MobileNetV2 model\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Resize to model input size\n",
    "            resized = cv2.resize(roi_image, self.input_size)\n",
    "\n",
    "            # Convert BGR to RGB (MobileNetV2 expects RGB)\n",
    "            rgb_image = cv2.cvtColor(resized, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "            # Expand dimensions for batch processing\n",
    "            img_array = np.expand_dims(rgb_image, axis=0)   #‚úÖ Model expects input shape of (1, H, W, 3)\n",
    "\n",
    "            # Apply MobileNetV2 preprocessing\n",
    "            preprocessed = preprocess_input(img_array.astype(np.float32))\n",
    "\n",
    "            return preprocessed\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Preprocessing error: {str(e)}\")\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FeHgUoCniRi0"
   },
   "source": [
    "# Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kc8aQjr5hxfC"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class ModelInference:\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.classes = Config.CLASSES\n",
    "        self.load_model()\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Load the trained MobileNetV2 model\"\"\"\n",
    "        try:\n",
    "            self.model = tf.keras.models.load_model(Config.MODEL_PATH)\n",
    "            print(f\"‚úÖ Model loaded successfully from {Config.MODEL_PATH}\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Error loading model: {str(e)}\")\n",
    "            self.model = None\n",
    "\n",
    "    def predict(self, preprocessed_image):\n",
    "        \"\"\"\n",
    "        Run inference on preprocessed image\n",
    "        Returns: (class_name, confidence, all_predictions)\n",
    "        \"\"\"\n",
    "        if self.model is None:\n",
    "            return \"invalid\", 0.0, None\n",
    "\n",
    "        try:\n",
    "            # Run prediction\n",
    "            predictions = self.model.predict(preprocessed_image, verbose=0)\n",
    "            print(f\"‚úÖ Predictions: {predictions}\")\n",
    "\n",
    "            # Get class with highest probability\n",
    "            predicted_class_idx = np.argmax(predictions[0])\n",
    "            print(f\"‚úÖ Predicted class index: {predicted_class_idx}\")\n",
    "\n",
    "            confidence = float(predictions[0][predicted_class_idx])\n",
    "            print(f\"‚úÖ Confidence: {confidence}\")\n",
    "\n",
    "            # Get class name\n",
    "            predicted_class = self.classes[predicted_class_idx]\n",
    "            print(f\"‚úÖ Predicted class: {predicted_class}\")\n",
    "\n",
    "            # Create prediction dictionary\n",
    "            all_predictions = {\n",
    "                class_name: float(prob)\n",
    "                for class_name, prob in zip(self.classes, predictions[0])\n",
    "            }\n",
    "            print(f\"‚úÖ All predictions: {all_predictions}\")\n",
    "\n",
    "            return predicted_class, confidence, all_predictions\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Inference error: {str(e)}\")\n",
    "            return \"invalid\", 0.0, None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "executionInfo": {
     "elapsed": 3244,
     "status": "ok",
     "timestamp": 1752341969317,
     "user": {
      "displayName": "Akshat",
      "userId": "02656238651709843440"
     },
     "user_tz": -330
    },
    "id": "uPjz6FjjiQh6",
    "outputId": "3d0bf118-cb68-4783-cfcf-394872ea9c9e"
   },
   "outputs": [],
   "source": [
    "from google.colab.patches import cv2_imshow\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# scissor\n",
    "image_path = \"/content/RPSENSE BACKEND/WIN_20250712_22_09_29_Pro.jpg\" # 90% paper WRONG\n",
    "#image_path = \"/content/RPSENSE BACKEND/scissor_back.jpg\" # 82% scissors\n",
    "#image_path = \"/content/RPSENSE BACKEND/scissor_front.jpg\"  #93% sc\n",
    "\n",
    "\n",
    "#paper\n",
    "#image_path = \"/content/RPSENSE BACKEND/paper_front.jpg\"  # 79 paper 16 scissor\n",
    "#image_path = \"/content/RPSENSE BACKEND/paper_back.jpg\"  # 100%\n",
    "\n",
    "#rock\n",
    "#image_path = \"/content/RPSENSE BACKEND/rock.jpg\" #99.50 %\n",
    "\n",
    "\n",
    "img = cv2.imread(image_path)\n",
    "\n",
    "preprocess = ImagePreprocessor()\n",
    "\n",
    "try:\n",
    "    # Preprocess for model\n",
    "    image = preprocess.preprocess_for_model(img)  # Shape: (1, 224, 224, 3)\n",
    "\n",
    "    # Remove batch dimension for display\n",
    "    image_to_display = np.squeeze(image, axis=0)  # Shape: (224, 224, 3)\n",
    "\n",
    "    # Convert preprocessed image back to uint8 for display (from [-1, 1] to [0, 255])\n",
    "    image_displayable = ((image_to_display + 1.0) * 127.5).astype(np.uint8)\n",
    "    cv2_imshow(image_displayable)\n",
    "\n",
    "    # Run inference\n",
    "    model_infer = ModelInference()\n",
    "    label, confidence, all_preds = model_infer.predict(image)\n",
    "\n",
    "    print(\"‚úÖ Prediction Results:\")\n",
    "    print(f\"Predicted class: {label}\")\n",
    "    print(f\"Confidence: {confidence:.2f}\")\n",
    "    print(\"All class probabilities:\")\n",
    "    print(\"\\nüìä Raw Predictions:\")\n",
    "    for cls, prob in all_preds.items():\n",
    "      bar = \"‚ñà\" * int(prob * 40)  # Visual bar\n",
    "      print(f\"{cls:10}: {prob:.4f} {bar}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error during prediction test: {str(e)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D12J1KMruqX2"
   },
   "source": [
    "# Postprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S6CXBOGPusP7"
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class PredictionPostprocessor:\n",
    "    def __init__(self):\n",
    "        self.confidence_threshold = Config.CONFIDENCE_THRESHOLD\n",
    "        self.frame_buffer = []\n",
    "        self.max_frames = Config.MAX_FRAMES_IN_WINDOW\n",
    "\n",
    "    def add_prediction(self, prediction, confidence, frame_data):\n",
    "        \"\"\"Add a prediction to the buffer\"\"\"\n",
    "        if confidence >= self.confidence_threshold:\n",
    "            self.frame_buffer.append(\n",
    "                {\n",
    "                    \"prediction\": prediction,\n",
    "                    \"confidence\": confidence,\n",
    "                    \"frame_data\": frame_data,\n",
    "                    \"timestamp\": frame_data.get(\"timestamp\"),\n",
    "                }\n",
    "            )\n",
    "\n",
    "\n",
    "    def get_aggregated_result(self):\n",
    "        \"\"\"\n",
    "        Aggregate predictions over the time window\n",
    "        Returns the most common prediction and the frame with highest confidence\n",
    "        \"\"\"\n",
    "        if not self.frame_buffer:\n",
    "            return None, None\n",
    "\n",
    "        # Count predictions\n",
    "        prediction_counts = Counter(\n",
    "            [frame[\"prediction\"] for frame in self.frame_buffer]\n",
    "        )\n",
    "\n",
    "        # Get most common prediction\n",
    "        most_common_prediction = prediction_counts.most_common(1)[0][0]\n",
    "\n",
    "        # Find frame with highest confidence for the most common prediction\n",
    "        best_frame = max(\n",
    "            [\n",
    "                frame\n",
    "                for frame in self.frame_buffer\n",
    "                if frame[\"prediction\"] == most_common_prediction\n",
    "            ],\n",
    "            key=lambda x: x[\"confidence\"],\n",
    "        )\n",
    "\n",
    "        # Calculate aggregation stats\n",
    "        total_frames = len(self.frame_buffer)\n",
    "        prediction_percentage = (\n",
    "            prediction_counts[most_common_prediction] / total_frames\n",
    "        ) * 100\n",
    "\n",
    "        result = {\n",
    "            \"final_prediction\": most_common_prediction,\n",
    "            \"confidence\": best_frame[\"confidence\"],\n",
    "            \"frame_count\": total_frames,\n",
    "            \"prediction_percentage\": prediction_percentage,\n",
    "            \"all_predictions\": dict(prediction_counts),\n",
    "            \"best_frame\": best_frame,\n",
    "        }\n",
    "\n",
    "        return result, best_frame\n",
    "\n",
    "    def clear_buffer(self):\n",
    "        \"\"\"Clear the prediction buffer\"\"\"\n",
    "        self.frame_buffer.clear()\n",
    "\n",
    "    def should_send_final_result(self):\n",
    "        \"\"\"Check if we have enough frames to send a final result\"\"\"\n",
    "        return len(self.frame_buffer) >= self.max_frames * 0.8  # 80% of expected frames\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yb48HoY7xoYT"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from google.colab.patches import cv2_imshow\n",
    "import cv2\n",
    "class FrameProcessor:\n",
    "    def __init__(self):\n",
    "        self.hand_detector = HandDetector()\n",
    "        self.preprocessor = ImagePreprocessor()\n",
    "        self.model_inference = ModelInference()\n",
    "        self.postprocessor = PredictionPostprocessor()\n",
    "\n",
    "    def process_frame(self, image, frame_metadata=None):\n",
    "        \"\"\"\n",
    "        Process a single frame through the entire pipeline\n",
    "        Returns: (status, real_time_result, should_send_final, final_result)\n",
    "        \"\"\"\n",
    "        timestamp = time.time()\n",
    "\n",
    "        # 1. Hand Detection\n",
    "        hand_status, hand_message, hand_landmarks = self.hand_detector.detect_hands(\n",
    "            image\n",
    "        )\n",
    "\n",
    "        if hand_status != \"success\":\n",
    "            return (\n",
    "                hand_status,\n",
    "                {\n",
    "                    \"status\": hand_status,\n",
    "                    \"message\": hand_message,\n",
    "                    \"timestamp\": timestamp,\n",
    "                },\n",
    "                False,\n",
    "                None,\n",
    "            )\n",
    "\n",
    "        try:\n",
    "            # 2. Extract hand ROI\n",
    "            roi_image, bbox = extract_hand_roi(image, hand_landmarks)\n",
    "            cv2_imshow(roi_image)\n",
    "            print(\"bbox\", bbox)\n",
    "\n",
    "            # 3. Preprocess for model\n",
    "            preprocessed_roi = self.preprocessor.preprocess_for_model(roi_image)\n",
    "            if preprocessed_roi is None:\n",
    "                return (\n",
    "                    \"error\",\n",
    "                    {\n",
    "                        \"status\": \"error\",\n",
    "                        \"message\": \"Preprocessing failed\",\n",
    "                        \"timestamp\": timestamp,\n",
    "                    },\n",
    "                    False,\n",
    "                    None,\n",
    "                )\n",
    "            # Remove batch dimension for display\n",
    "            image_to_display = np.squeeze(preprocessed_roi, axis=0)  # Shape: (224, 224, 3)\n",
    "\n",
    "            # Convert preprocessed image back to uint8 for display (from [-1, 1] to [0, 255])\n",
    "            image_displayable = ((image_to_display + 1.0) * 127.5).astype(np.uint8)\n",
    "            cv2_imshow(image_displayable)\n",
    "\n",
    "            # 4. Run inference\n",
    "            prediction, confidence, all_predictions = self.model_inference.predict(\n",
    "                preprocessed_roi\n",
    "            )\n",
    "            print(prediction)\n",
    "            print(confidence)\n",
    "            print(all_predictions)\n",
    "\n",
    "\n",
    "            # 5. Create frame data\n",
    "            frame_data = {\n",
    "                \"timestamp\": timestamp,\n",
    "                \"bbox\": bbox,\n",
    "                \"original_image\": image.copy(),\n",
    "                \"roi\": roi_image,\n",
    "                \"metadata\": frame_metadata or {},\n",
    "            }\n",
    "\n",
    "            # 6. Add to postprocessor buffer\n",
    "            self.postprocessor.add_prediction(prediction, confidence, frame_data)\n",
    "\n",
    "            # 7. Create overlay image for real-time feedback\n",
    "            overlay_image = image.copy()\n",
    "            overlay_image = draw_prediction_overlay(\n",
    "                overlay_image, bbox, prediction, confidence\n",
    "            )\n",
    "            cv2_imshow(overlay_image)\n",
    "            overlay_base64 = encode_frame_to_base64(overlay_image)\n",
    "\n",
    "            # 8. Real-time result\n",
    "            real_time_result = {\n",
    "                \"status\": \"success\",\n",
    "                \"prediction\": prediction,\n",
    "                \"confidence\": confidence,\n",
    "                \"all_predictions\": all_predictions,\n",
    "                \"overlay_image\": overlay_base64,\n",
    "                \"timestamp\": timestamp,\n",
    "                \"buffer_size\": len(self.postprocessor.frame_buffer),\n",
    "            }\n",
    "\n",
    "            # 9. Check if we should send final result\n",
    "            should_send_final = True\n",
    "            final_result = None\n",
    "\n",
    "            if should_send_final:\n",
    "                aggregated_result, best_frame = (\n",
    "                    self.postprocessor.get_aggregated_result()\n",
    "                )\n",
    "                if aggregated_result and best_frame:\n",
    "                    # Create final overlay with best frame\n",
    "                    final_overlay = best_frame[\"frame_data\"][\"original_image\"].copy()\n",
    "                    final_overlay = draw_prediction_overlay(\n",
    "                        final_overlay,\n",
    "                        best_frame[\"frame_data\"][\"bbox\"],\n",
    "                        aggregated_result[\"final_prediction\"],\n",
    "                        aggregated_result[\"confidence\"],\n",
    "                    )\n",
    "                    final_overlay_base64 = encode_frame_to_base64(final_overlay)\n",
    "\n",
    "                    final_result = {\n",
    "                        \"status\": \"final_result\",\n",
    "                        \"final_prediction\": aggregated_result[\"final_prediction\"],\n",
    "                        \"confidence\": aggregated_result[\"confidence\"],\n",
    "                        \"frame_count\": aggregated_result[\"frame_count\"],\n",
    "                        \"prediction_percentage\": aggregated_result[\n",
    "                            \"prediction_percentage\"\n",
    "                        ],\n",
    "                        \"all_predictions\": aggregated_result[\"all_predictions\"],\n",
    "                        \"final_overlay_image\": final_overlay_base64,\n",
    "                        \"timestamp\": timestamp,\n",
    "                    }\n",
    "\n",
    "                    # Clear buffer after sending final result\n",
    "                    self.postprocessor.clear_buffer()\n",
    "\n",
    "            return \"success\", real_time_result, should_send_final, final_result\n",
    "\n",
    "        except Exception as e:\n",
    "            return (\n",
    "                \"error\",\n",
    "                {\n",
    "                    \"status\": \"error\",\n",
    "                    \"message\": f\"Processing error: {str(e)}\",\n",
    "                    \"timestamp\": timestamp,\n",
    "                },\n",
    "                False,\n",
    "                None,\n",
    "            )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "output_embedded_package_id": "1meKyAumlm2q5QRt9N1r9IHPhnxBmTEm3"
    },
    "id": "rou2-5wey_VL",
    "outputId": "6dfc7110-55c5-4445-a1fd-a4abfca5796a"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Image path to test\n",
    "image_path = \"/content/RPSENSE BACKEND/scissor_back.jpg\"\n",
    "\n",
    "\n",
    "# Load image using OpenCV\n",
    "image = cv2.imread(image_path)\n",
    "if image is None:\n",
    "    print(f\"‚ùå Failed to load image from: {image_path}\")\n",
    "else:\n",
    "    # Initialize FrameProcessor\n",
    "    processor = FrameProcessor()\n",
    "\n",
    "    # Process the frame\n",
    "    status, real_time_result, should_send_final, final_result = processor.process_frame(image)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"\\n‚úÖ Processing Status: {status}\")\n",
    "    print(\"üïí Timestamp:\", real_time_result.get(\"timestamp\"))\n",
    "\n",
    "    if status == \"success\":\n",
    "        print(\"üîÆ Prediction:\", real_time_result.get(\"prediction\"))\n",
    "        print(\"üìà Confidence:\", f\"{real_time_result.get('confidence'):.2f}\")\n",
    "        print(\"üìä All Probabilities:\", real_time_result.get(\"all_predictions\"))\n",
    "\n",
    "        if should_send_final:\n",
    "            print(\"\\n‚úÖ Final Aggregated Result:\")\n",
    "            print(\"üîÅ Final Prediction:\", final_result[\"final_prediction\"])\n",
    "            print(\"üìà Confidence:\", f\"{final_result['confidence']:.2f}\")\n",
    "            print(\"üß† Prediction Percentage:\", final_result[\"prediction_percentage\"])\n",
    "            print(\"üì¶ Frame Count Used:\", final_result[\"frame_count\"])\n",
    "\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Failed to process frame:\", real_time_result.get(\"message\"))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyM3Z7ZgCSkYf+2HuQoeJjzN",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
